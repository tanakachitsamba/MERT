{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torchaudio datasets\n",
        "!pip install nnAudio"
      ],
      "metadata": {
        "id": "a4kpkjI-wJqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "from sklearn.decomposition import PCA\n",
        "from torch import nn\n",
        "from transformers import AutoModel, Wav2Vec2FeatureExtractor\n",
        "from datasets import load_dataset\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "class CustomResampler(nn.Module):\n",
        "    def __init__(self, orig_freq, new_freq):\n",
        "        super(CustomResampler, self).__init__()\n",
        "        self.orig_freq = orig_freq\n",
        "        self.new_freq = new_freq\n",
        "        self.resampler = T.Resample(orig_freq, new_freq)\n",
        "\n",
        "    def forward(self, waveform):\n",
        "        resampled = self.resampler(waveform)\n",
        "        return resampled\n",
        "\n",
        "def save_waveform(tensor, filename):\n",
        "    torchaudio.save(filename, tensor, sample_rate=resample_rate)\n",
        "\n",
        "def create_waveform_from_hidden_state(hidden_state, sample_rate):\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_dim_data = pca.fit_transform(hidden_state.T).T\n",
        "    normalized_waveform = (reduced_dim_data - reduced_dim_data.min()) / (reduced_dim_data.max() - reduced_dim_data.min())\n",
        "    return torch.tensor(normalized_waveform, dtype=torch.float32)\n",
        "\n",
        "model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True)\n",
        "processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True)\n",
        "\n",
        "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
        "dataset = dataset.sort(\"id\")\n",
        "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
        "\n",
        "resample_rate = processor.sampling_rate\n",
        "if resample_rate != sampling_rate:\n",
        "    print(f'Setting rate from {sampling_rate} to {resample_rate}')\n",
        "    resampler = CustomResampler(sampling_rate, resample_rate)\n",
        "else:\n",
        "    resampler = None\n",
        "\n",
        "if resampler is None:\n",
        "    input_audio = dataset[0][\"audio\"][\"array\"]\n",
        "else:\n",
        "    input_audio = resampler(torch.from_numpy(dataset[0][\"audio\"][\"array\"]).float())\n",
        "\n",
        "inputs = processor(input_audio.squeeze(), sampling_rate=resample_rate, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "all_layer_hidden_states = torch.stack(outputs.hidden_states).squeeze()\n",
        "\n",
        "output_dir = \"waveforms\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "for i, hidden_state in enumerate(all_layer_hidden_states):\n",
        "    waveform_filename = os.path.join(output_dir, f\"hidden_state_{i}.wav\")\n",
        "    waveform = create_waveform_from_hidden_state(hidden_state.detach().numpy(), resample_rate)\n",
        "    save_waveform(waveform, waveform_filename)\n",
        "\n",
        "print(f\"Waveform files saved in {output_dir}\")"
      ],
      "metadata": {
        "id": "BXNm7QQq3qFf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}